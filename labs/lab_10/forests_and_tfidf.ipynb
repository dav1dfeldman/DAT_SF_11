{
 "metadata": {
  "name": "",
<<<<<<< HEAD
  "signature": "sha256:8bd0cd99e0de072a3ce2ebc477d605cba7bc68c51cb4ab88f6c5e7b2705e1ff4"
=======
  "signature": "sha256:e5f4646f088251c5bb5aa3dd7ef1418aa0f09b6f459abc8d1045599453dbc8c3"
>>>>>>> 5ff02bdacae2a66fc9c21becf1d686e529da36c3
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using tf-idf and random forests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Newsgroups Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will take a look at some of the twenty newsgroups dataset, another common dataset for classification. Note that the data is fetched from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "# We will use four of the twenty newsgroups\n",
      "categories = ['alt.atheism',\n",
      "              'talk.religion.misc',\n",
      "              'comp.graphics',\n",
      "              'sci.space']\n",
      "\n",
      "twenty_train_subset = fetch_20newsgroups(subset='train', categories=categories)\n",
      "twenty_test_subset = fetch_20newsgroups(subset='test', categories=categories)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have lists of messages (as strings) in the `.data` members."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datasets.get_data_home()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'/Users/David/scikit_learn_data'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Features from text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some ways to generate features from the text:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Count Vectorizer\n",
      "Count Vectorizer is the easiest text processing utility to understand, it simply counts the occurances of non-stopwords. (what is a stopword?)<br>\n",
      "First, let's check out our data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print twenty_train_subset.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: rych@festival.ed.ac.uk (R Hawkes)\n",
        "Subject: 3DS: Where did all the texture rules go?\n",
        "Lines: 21\n",
        "\n",
        "Hi,\n",
        "\n",
        "I've noticed that if you only save a model (with all your mapping planes\n",
        "positioned carefully) to a .3DS file that when you reload it after restarting\n",
        "3DS, they are given a default position and orientation.  But if you save\n",
        "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
        "know why this information is not stored in the .3DS file?  Nothing is\n",
        "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
        "I'd like to be able to read the texture rule information, does anyone have \n",
        "the format for the .PRJ file?\n",
        "\n",
        "Is the .CEL file format available from somewhere?\n",
        "\n",
        "Rych\n",
        "\n",
        "======================================================================\n",
        "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
        "Virtual Environment Laboratory\n",
        "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
        "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
        "======================================================================\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer(stop_words='english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "one_message = vectorizer.fit_transform(twenty_train_subset.data[0:3]).todense().tolist()[0]\n",
      "zip(vectorizer.get_feature_names(),one_message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(u'0150', 1L),\n",
        " (u'02', 0L),\n",
        " (u'020359', 0L),\n",
        " (u'04', 0L),\n",
        " (u'07', 0L),\n",
        " (u'08', 0L),\n",
        " (u'15', 0L),\n",
        " (u'19', 0L),\n",
        " (u'1970', 0L),\n",
        " (u'1993', 0L),\n",
        " (u'1993apr19', 0L),\n",
        " (u'20', 0L),\n",
        " (u'20apr199301460499', 0L),\n",
        " (u'21', 1L),\n",
        " (u'23', 0L),\n",
        " (u'2400x4', 0L),\n",
        " (u'245', 0L),\n",
        " (u'26996', 0L),\n",
        " (u'31', 2L),\n",
        " (u'3205', 0L),\n",
        " (u'3426', 1L),\n",
        " (u'3ds', 4L),\n",
        " (u'4366', 0L),\n",
        " (u'44', 2L),\n",
        " (u'55', 0L),\n",
        " (u'604', 0L),\n",
        " (u'650', 1L),\n",
        " (u'667', 1L),\n",
        " (u'_perijoves_', 0L),\n",
        " (u'able', 1L),\n",
        " (u'ac', 2L),\n",
        " (u'access', 0L),\n",
        " (u'actually', 0L),\n",
        " (u'almanac', 0L),\n",
        " (u'apoapsis', 0L),\n",
        " (u'apr', 0L),\n",
        " (u'article', 0L),\n",
        " (u'available', 1L),\n",
        " (u'b645zaw', 0L),\n",
        " (u'backing', 0L),\n",
        " (u'barring', 0L),\n",
        " (u'bc', 0L),\n",
        " (u'biblical', 0L),\n",
        " (u'brader', 0L),\n",
        " (u'british', 0L),\n",
        " (u'bunch', 0L),\n",
        " (u'ca', 0L),\n",
        " (u'canada', 0L),\n",
        " (u'carefully', 1L),\n",
        " (u'carried', 0L),\n",
        " (u'cel', 1L),\n",
        " (u'central', 0L),\n",
        " (u'centuries', 0L),\n",
        " (u'children', 0L),\n",
        " (u'circa', 0L),\n",
        " (u'cites', 0L),\n",
        " (u'columbia', 0L),\n",
        " (u'com', 0L),\n",
        " (u'comet', 0L),\n",
        " (u'contrary', 0L),\n",
        " (u'corruption', 0L),\n",
        " (u'couldn', 0L),\n",
        " (u'default', 1L),\n",
        " (u'delusional', 0L),\n",
        " (u'demonstrating', 0L),\n",
        " (u'dept', 1L),\n",
        " (u'deranged', 0L),\n",
        " (u'did', 1L),\n",
        " (u'does', 2L),\n",
        " (u'ed', 2L),\n",
        " (u'edinburgh', 1L),\n",
        " (u'edu', 0L),\n",
        " (u'email', 1L),\n",
        " (u'enclosed', 0L),\n",
        " (u'environment', 1L),\n",
        " (u'evidence', 0L),\n",
        " (u'evil', 0L),\n",
        " (u'explicitly', 1L),\n",
        " (u'f208', 0L),\n",
        " (u'fanatic', 0L),\n",
        " (u'fax', 1L),\n",
        " (u'feb', 0L),\n",
        " (u'festival', 2L),\n",
        " (u'fidonet', 0L),\n",
        " (u'figure', 0L),\n",
        " (u'file', 6L),\n",
        " (u'fisher', 0L),\n",
        " (u'folks', 0L),\n",
        " (u'format', 2L),\n",
        " (u'frog', 0L),\n",
        " (u'fruitcakes', 0L),\n",
        " (u'given', 1L),\n",
        " (u'got', 0L),\n",
        " (u'gotten', 0L),\n",
        " (u'gt', 0L),\n",
        " (u'hawkes', 2L),\n",
        " (u'hi', 1L),\n",
        " (u'hisse', 0L),\n",
        " (u'holocaust', 0L),\n",
        " (u'home', 0L),\n",
        " (u'information', 2L),\n",
        " (u'internet', 0L),\n",
        " (u'island', 0L),\n",
        " (u'jg', 0L),\n",
        " (u'jgarland', 0L),\n",
        " (u'jim', 0L),\n",
        " (u'jones', 0L),\n",
        " (u'jupiter', 0L),\n",
        " (u'just', 0L),\n",
        " (u'kean', 0L),\n",
        " (u'ken', 0L),\n",
        " (u'killed', 0L),\n",
        " (u'kmcvay', 0L),\n",
        " (u'know', 1L),\n",
        " (u'koresh', 0L),\n",
        " (u'laboratory', 1L),\n",
        " (u'ladysmith', 0L),\n",
        " (u'language', 0L),\n",
        " (u'learned', 0L),\n",
        " (u'like', 1L),\n",
        " (u'lines', 1L),\n",
        " (u'lot', 0L),\n",
        " (u'mail', 0L),\n",
        " (u'mania', 0L),\n",
        " (u'manual', 1L),\n",
        " (u'mapping', 1L),\n",
        " (u'mark', 0L),\n",
        " (u'mb', 0L),\n",
        " (u'mcvay', 0L),\n",
        " (u'mean', 0L),\n",
        " (u'message', 0L),\n",
        " (u'messenger', 0L),\n",
        " (u'model', 1L),\n",
        " (u'msb', 0L),\n",
        " (u'msged', 0L),\n",
        " (u'mun', 0L),\n",
        " (u'n103', 0L),\n",
        " (u'neccessary', 0L),\n",
        " (u'newtout', 0L),\n",
        " (u'nope', 0L),\n",
        " (u'noticed', 1L),\n",
        " (u'old', 0L),\n",
        " (u'oneb', 0L),\n",
        " (u'orbit', 0L),\n",
        " (u'org', 0L),\n",
        " (u'organization', 0L),\n",
        " (u'orientation', 2L),\n",
        " (u'p201', 0L),\n",
        " (u'perew', 0L),\n",
        " (u'periapsis', 0L),\n",
        " (u'perijove', 0L),\n",
        " (u'planes', 1L),\n",
        " (u'position', 1L),\n",
        " (u'positioned', 1L),\n",
        " (u'positions', 1L),\n",
        " (u'preserved', 1L),\n",
        " (u'prj', 3L),\n",
        " (u'psychology', 1L),\n",
        " (u'public', 0L),\n",
        " (u'read', 1L),\n",
        " (u'reload', 1L),\n",
        " (u'restarting', 1L),\n",
        " (u'rotten', 0L),\n",
        " (u'rule', 1L),\n",
        " (u'rules', 2L),\n",
        " (u'rych', 3L),\n",
        " (u'rycharde', 1L),\n",
        " (u'ryugen', 0L),\n",
        " (u'said', 1L),\n",
        " (u'salute', 0L),\n",
        " (u'satisfy', 0L),\n",
        " (u'save', 2L),\n",
        " (u'saving', 1L),\n",
        " (u'say', 0L),\n",
        " (u'says', 0L),\n",
        " (u'sco', 0L),\n",
        " (u'sender', 0L),\n",
        " (u'serving', 0L),\n",
        " (u'simply', 0L),\n",
        " (u'sorry', 0L),\n",
        " (u'sq', 0L),\n",
        " (u'stephen', 0L),\n",
        " (u'stored', 1L),\n",
        " (u'subject', 1L),\n",
        " (u'sure', 0L),\n",
        " (u'surprised', 0L),\n",
        " (u'talking', 0L),\n",
        " (u'tape', 0L),\n",
        " (u'tel', 1L),\n",
        " (u'temporary', 0L),\n",
        " (u'texture', 3L),\n",
        " (u'things', 0L),\n",
        " (u'thought', 0L),\n",
        " (u'time', 0L),\n",
        " (u'ucs', 0L),\n",
        " (u'uk', 2L),\n",
        " (u'univ', 1L),\n",
        " (u'unlikely', 0L),\n",
        " (u'used', 0L),\n",
        " (u'usenet', 0L),\n",
        " (u'uta', 0L),\n",
        " (u'utarlg', 0L),\n",
        " (u'v32', 0L),\n",
        " (u'vancouver', 0L),\n",
        " (u've', 1L),\n",
        " (u'virtual', 1L),\n",
        " (u'writes', 0L),\n",
        " (u'xenix', 0L),\n",
        " (u'z1', 0L)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This makes a matrix of word counts, where each row is a document and each column is the word, the cell matrix[document, word] contains the count of word in document.\n",
      "<br><br>\n",
      "Now try this with the whole training subset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<2034x33815 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 233470 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default this returns a sparse matrix, which will save memory.\n",
      "\n",
      "Also, notice our stop words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_stop_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "frozenset({'a',\n",
        "           'about',\n",
        "           'above',\n",
        "           'across',\n",
        "           'after',\n",
        "           'afterwards',\n",
        "           'again',\n",
        "           'against',\n",
        "           'all',\n",
        "           'almost',\n",
        "           'alone',\n",
        "           'along',\n",
        "           'already',\n",
        "           'also',\n",
        "           'although',\n",
        "           'always',\n",
        "           'am',\n",
        "           'among',\n",
        "           'amongst',\n",
        "           'amoungst',\n",
        "           'amount',\n",
        "           'an',\n",
        "           'and',\n",
        "           'another',\n",
        "           'any',\n",
        "           'anyhow',\n",
        "           'anyone',\n",
        "           'anything',\n",
        "           'anyway',\n",
        "           'anywhere',\n",
        "           'are',\n",
        "           'around',\n",
        "           'as',\n",
        "           'at',\n",
        "           'back',\n",
        "           'be',\n",
        "           'became',\n",
        "           'because',\n",
        "           'become',\n",
        "           'becomes',\n",
        "           'becoming',\n",
        "           'been',\n",
        "           'before',\n",
        "           'beforehand',\n",
        "           'behind',\n",
        "           'being',\n",
        "           'below',\n",
        "           'beside',\n",
        "           'besides',\n",
        "           'between',\n",
        "           'beyond',\n",
        "           'bill',\n",
        "           'both',\n",
        "           'bottom',\n",
        "           'but',\n",
        "           'by',\n",
        "           'call',\n",
        "           'can',\n",
        "           'cannot',\n",
        "           'cant',\n",
        "           'co',\n",
        "           'con',\n",
        "           'could',\n",
        "           'couldnt',\n",
        "           'cry',\n",
        "           'de',\n",
        "           'describe',\n",
        "           'detail',\n",
        "           'do',\n",
        "           'done',\n",
        "           'down',\n",
        "           'due',\n",
        "           'during',\n",
        "           'each',\n",
        "           'eg',\n",
        "           'eight',\n",
        "           'either',\n",
        "           'eleven',\n",
        "           'else',\n",
        "           'elsewhere',\n",
        "           'empty',\n",
        "           'enough',\n",
        "           'etc',\n",
        "           'even',\n",
        "           'ever',\n",
        "           'every',\n",
        "           'everyone',\n",
        "           'everything',\n",
        "           'everywhere',\n",
        "           'except',\n",
        "           'few',\n",
        "           'fifteen',\n",
        "           'fify',\n",
        "           'fill',\n",
        "           'find',\n",
        "           'fire',\n",
        "           'first',\n",
        "           'five',\n",
        "           'for',\n",
        "           'former',\n",
        "           'formerly',\n",
        "           'forty',\n",
        "           'found',\n",
        "           'four',\n",
        "           'from',\n",
        "           'front',\n",
        "           'full',\n",
        "           'further',\n",
        "           'get',\n",
        "           'give',\n",
        "           'go',\n",
        "           'had',\n",
        "           'has',\n",
        "           'hasnt',\n",
        "           'have',\n",
        "           'he',\n",
        "           'hence',\n",
        "           'her',\n",
        "           'here',\n",
        "           'hereafter',\n",
        "           'hereby',\n",
        "           'herein',\n",
        "           'hereupon',\n",
        "           'hers',\n",
        "           'herself',\n",
        "           'him',\n",
        "           'himself',\n",
        "           'his',\n",
        "           'how',\n",
        "           'however',\n",
        "           'hundred',\n",
        "           'i',\n",
        "           'ie',\n",
        "           'if',\n",
        "           'in',\n",
        "           'inc',\n",
        "           'indeed',\n",
        "           'interest',\n",
        "           'into',\n",
        "           'is',\n",
        "           'it',\n",
        "           'its',\n",
        "           'itself',\n",
        "           'keep',\n",
        "           'last',\n",
        "           'latter',\n",
        "           'latterly',\n",
        "           'least',\n",
        "           'less',\n",
        "           'ltd',\n",
        "           'made',\n",
        "           'many',\n",
        "           'may',\n",
        "           'me',\n",
        "           'meanwhile',\n",
        "           'might',\n",
        "           'mill',\n",
        "           'mine',\n",
        "           'more',\n",
        "           'moreover',\n",
        "           'most',\n",
        "           'mostly',\n",
        "           'move',\n",
        "           'much',\n",
        "           'must',\n",
        "           'my',\n",
        "           'myself',\n",
        "           'name',\n",
        "           'namely',\n",
        "           'neither',\n",
        "           'never',\n",
        "           'nevertheless',\n",
        "           'next',\n",
        "           'nine',\n",
        "           'no',\n",
        "           'nobody',\n",
        "           'none',\n",
        "           'noone',\n",
        "           'nor',\n",
        "           'not',\n",
        "           'nothing',\n",
        "           'now',\n",
        "           'nowhere',\n",
        "           'of',\n",
        "           'off',\n",
        "           'often',\n",
        "           'on',\n",
        "           'once',\n",
        "           'one',\n",
        "           'only',\n",
        "           'onto',\n",
        "           'or',\n",
        "           'other',\n",
        "           'others',\n",
        "           'otherwise',\n",
        "           'our',\n",
        "           'ours',\n",
        "           'ourselves',\n",
        "           'out',\n",
        "           'over',\n",
        "           'own',\n",
        "           'part',\n",
        "           'per',\n",
        "           'perhaps',\n",
        "           'please',\n",
        "           'put',\n",
        "           'rather',\n",
        "           're',\n",
        "           'same',\n",
        "           'see',\n",
        "           'seem',\n",
        "           'seemed',\n",
        "           'seeming',\n",
        "           'seems',\n",
        "           'serious',\n",
        "           'several',\n",
        "           'she',\n",
        "           'should',\n",
        "           'show',\n",
        "           'side',\n",
        "           'since',\n",
        "           'sincere',\n",
        "           'six',\n",
        "           'sixty',\n",
        "           'so',\n",
        "           'some',\n",
        "           'somehow',\n",
        "           'someone',\n",
        "           'something',\n",
        "           'sometime',\n",
        "           'sometimes',\n",
        "           'somewhere',\n",
        "           'still',\n",
        "           'such',\n",
        "           'system',\n",
        "           'take',\n",
        "           'ten',\n",
        "           'than',\n",
        "           'that',\n",
        "           'the',\n",
        "           'their',\n",
        "           'them',\n",
        "           'themselves',\n",
        "           'then',\n",
        "           'thence',\n",
        "           'there',\n",
        "           'thereafter',\n",
        "           'thereby',\n",
        "           'therefore',\n",
        "           'therein',\n",
        "           'thereupon',\n",
        "           'these',\n",
        "           'they',\n",
        "           'thick',\n",
        "           'thin',\n",
        "           'third',\n",
        "           'this',\n",
        "           'those',\n",
        "           'though',\n",
        "           'three',\n",
        "           'through',\n",
        "           'throughout',\n",
        "           'thru',\n",
        "           'thus',\n",
        "           'to',\n",
        "           'together',\n",
        "           'too',\n",
        "           'top',\n",
        "           'toward',\n",
        "           'towards',\n",
        "           'twelve',\n",
        "           'twenty',\n",
        "           'two',\n",
        "           'un',\n",
        "           'under',\n",
        "           'until',\n",
        "           'up',\n",
        "           'upon',\n",
        "           'us',\n",
        "           'very',\n",
        "           'via',\n",
        "           'was',\n",
        "           'we',\n",
        "           'well',\n",
        "           'were',\n",
        "           'what',\n",
        "           'whatever',\n",
        "           'when',\n",
        "           'whence',\n",
        "           'whenever',\n",
        "           'where',\n",
        "           'whereafter',\n",
        "           'whereas',\n",
        "           'whereby',\n",
        "           'wherein',\n",
        "           'whereupon',\n",
        "           'wherever',\n",
        "           'whether',\n",
        "           'which',\n",
        "           'while',\n",
        "           'whither',\n",
        "           'who',\n",
        "           'whoever',\n",
        "           'whole',\n",
        "           'whom',\n",
        "           'whose',\n",
        "           'why',\n",
        "           'will',\n",
        "           'with',\n",
        "           'within',\n",
        "           'without',\n",
        "           'would',\n",
        "           'yet',\n",
        "           'you',\n",
        "           'your',\n",
        "           'yours',\n",
        "           'yourself',\n",
        "           'yourselves'})"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###n-gram\n",
      "The basic idea of n-gram is to take a sequence of objects and make sense of it. Take, for example:<br><br>\n",
      "\n",
      "`I am Sam,`<br>\n",
      "`Sam I am,`<br>\n",
      "`Do you like green eggs and ham?`<br><br>\n",
      "\n",
      "To gram this we will extract all sequences of length `n` like so (for `n=3`):<br><br>\n",
      "`(i,am,sam),(am,sam,sam),(sam,sam,i),(sam,i,am),...`<br><br>\n",
      "\n",
      "scikit gives us the option to use n-grams as features their extraction module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Include every 1-gram, 2-gram, and 3-gram\n",
      "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this heavily inflates feature set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<2034x688611 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 1437560 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###TF-IDF"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Additionally, we could use a tf-idf representation, which stands for Term Frequency, Inverse Document Frequency.\n",
      "\n",
      "This value is the product of two intermediate values, the Term Frequency and the Inverse Document Frequency.\n",
      "\n",
      "The Term Frequency is equivalent to the `CountVectorizer` features, the number of times or count that a word appear in the document. This is our most basic representation of text.\n",
      "\n",
      "To establish Inverse Document Frequency, first let's define Document Frequency. This is the percentage of documents that a particular word appears in. For example, the word `the` might appear in 100% of documents, while words like `Syria` would likely have low document frequency. Inverse Document Frequency is simply 1 / Document Frequency (although often the log is also taken).\n",
      "\n",
      "Let, $D$ be the set of all documents:\n",
      "$$\n",
      "idf(word,D) = \\log \\frac{N}{|\\{d \\in D : t \\in d\\}|}\n",
      "$$\n",
      "\n",
      "Notice that this has the neat property that if a word occurs in ALL documents, $idf = 0$ so this naturally controls for stop words \n",
      "\n",
      "So tf-idf is Term Frequency * Inverse Document Frequency, or similar to Term Frequency / Document Frequency. The intuition is that words that have high weight are those that appear a lot in this document and/or appear in very few other documents (somehow unique to this document).\n",
      "\n",
      "Example:\n",
      "Let, $d_1 = $\"i am sam sam i am\", and $d_2 = $ \"i do not like them sam i am\", then:\n",
      "$$\n",
      "tf(like,d_2) = 1\n",
      "$$\n",
      "$$\n",
      "idf(like,D) = \\log \\frac{2}{1} = 0.3010\n",
      "$$\n",
      "$$\n",
      "tfidf(like,d_2) = 1*0.3010\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer()\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1.01 s, sys: 101 ms, total: 1.11 s\n",
        "Wall time: 1.08 s\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can put this together with our other tricks as well...but notice the running time hit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 9.81 s, sys: 447 ms, total: 10.3 s\n",
        "Wall time: 10.2 s\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[READ THE DOCS!](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use predict using our 20-newsgroup dataset above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(stop_words='english')\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "tree_model = DecisionTreeClassifier()\n",
      "print cross_val_score(tree_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.78316746695\n",
        "CPU times: user 17 s, sys: 1.04 s, total: 18.1 s\n",
        "Wall time: 18.4 s\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
<<<<<<< HEAD
      "est = RandomForestClassifier\n",
      "rf_model = est(n_estimators=20)\n",
=======
      "rf_model = RandomForestClassifier(n_estimators=60)\n",
>>>>>>> 5ff02bdacae2a66fc9c21becf1d686e529da36c3
      "print cross_val_score(rf_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.864280049847\n",
        "CPU times: user 6.46 s, sys: 937 ms, total: 7.4 s\n",
        "Wall time: 7.4 s\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Important Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This prints the top 10 most important features\n",
      "rf_model.fit(X_train.toarray(),twenty_train_subset.target)\n",
      "sorted(zip(rf_model.feature_importances_, vectorizer.get_feature_names()), reverse=True)[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[(0.016297295781772071, u'graphics'),\n",
        " (0.013987469355812082, u'space'),\n",
        " (0.0098898265219914006, u'writes'),\n",
        " (0.0098750506412186266, u'keith'),\n",
        " (0.009649831085772699, u'god'),\n",
        " (0.0087980403669394684, u'nasa'),\n",
        " (0.0063072660960981509, u'henry'),\n",
        " (0.0061421109804954176, u'prb'),\n",
        " (0.0059376093559033866, u'atheists'),\n",
        " (0.005884686595414885, u'christian'),\n",
        " (0.0058823522010488846, u'orbit'),\n",
        " (0.0056695897785144526, u'cco'),\n",
        " (0.0054145555968623527, u'allan'),\n",
        " (0.0054129024460646148, u'article'),\n",
        " (0.0052276798254129323, u'moon'),\n",
        " (0.005200440738675355, u'say'),\n",
        " (0.005091865754611272, u'edu'),\n",
        " (0.004588195720287444, u'pat'),\n",
        " (0.0042222954696367835, u'thanks'),\n",
        " (0.0041815145380713226, u'image')]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Your Turn:\n",
      "* Calculate the learning curve for increasing values of `n_estimators`\n",
      "* Plot the confusion matrix\n",
      "* Report recall, precision and f-score for each class using your best estimator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Solution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(stop_words='english')\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)\n",
      "#IMPORTANT, you should only transform you tested data, remember that in practice \n",
      "#you will not be able to retrain your model each time you get new data, so we \n",
      "#must simulate this constraint\n",
      "X_test = vectorizer.transform(twenty_test_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.learning_curve import learning_curve\n",
      "\n",
      "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
      "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
      "    \"\"\"\n",
      "    Generate a simple plot of the test and traning learning curve.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "        An object of that type which is cloned for each validation.\n",
      "\n",
      "    title : string\n",
      "        Title for the chart.\n",
      "\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vector, where n_samples is the number of samples and\n",
      "        n_features is the number of features.\n",
      "\n",
      "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
      "        Target relative to X for classification or regression;\n",
      "        None for unsupervised learning.\n",
      "\n",
      "    ylim : tuple, shape (ymin, ymax), optional\n",
      "        Defines minimum and maximum yvalues plotted.\n",
      "\n",
      "    cv : integer, cross-validation generator, optional\n",
      "        If an integer is passed, it is the number of folds (defaults to 3).\n",
      "        Specific cross-validation objects can be passed, see\n",
      "        sklearn.cross_validation module for the list of possible objects\n",
      "\n",
      "    n_jobs : integer, optional\n",
      "        Number of jobs to run in parallel (default 1).\n",
      "    \"\"\"\n",
      "    plt.figure()\n",
      "    plt.title(title)\n",
      "    if ylim is not None:\n",
      "        plt.ylim(*ylim)\n",
      "    plt.xlabel(\"Training examples\")\n",
      "    plt.ylabel(\"Score\")\n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
      "    train_scores_mean = np.mean(train_scores, axis=1)\n",
      "    train_scores_std = np.std(train_scores, axis=1)\n",
      "    test_scores_mean = np.mean(test_scores, axis=1)\n",
      "    test_scores_std = np.std(test_scores, axis=1)\n",
      "    plt.grid()\n",
      "\n",
      "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
      "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
      "                     color=\"r\")\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
      "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
      "             label=\"Training score\")\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
      "             label=\"Cross-validation score\")\n",
      "\n",
      "    plt.legend(loc=\"best\")\n",
      "    return plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#this is an example of 1 learning curve for 1 model, try a few more.\n",
      "_ = plot_learning_curve(RandomForestClassifier(n_estimators=100),'test',X_train.toarray(),twenty_train_subset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_model = RandomForestClassifier(n_estimators=100,max_depth=15,criterion='entropy')\n",
      "rf_model.fit(X_train.toarray(),twenty_train_subset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = rf_model.predict(X_test.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(confusion_matrix(y, y_pred),\n",
      "               cmap=plt.cm.binary, interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.xlabel('true value')\n",
      "    plt.ylabel('predicted value')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_confusion_matrix( y_pred,twenty_test_subset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "print classification_report(twenty_test_subset.target,y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = []\n",
      "def learning_curve(n):\n",
      "    for i in range(0, n):\n",
      "        rf_model = RandomForestClassifier(n_estimators=n)\n",
      "        score = cross_val_score(rf_model, X_train.toarray(), twenty_train_subset.target).mean()\n",
      "        scores.append(score)\n",
      "learning_curve(12)\n",
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.83775958379447679, 0.85937958420115435, 0.84117422360879945, 0.83674288967706889, 0.84361574191074595, 0.84070727052487559, 0.86185160332662314, 0.8475670509770431, 0.8505147377074419, 0.83923887373530504, 0.86086758866298918, 0.85151182415258553]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "pd.DataFrame(scores).plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x106f9d190>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8C0WiyUh2IBvsVgsCeH/ATjO3iaU7lzhAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x106fb6b50>"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classification_report(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'y_test' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-24-8570c1987f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}